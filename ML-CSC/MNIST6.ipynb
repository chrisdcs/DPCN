{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import grad\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold(x, lambd):\n",
    "    return torch.sign(x) * torch.max(torch.abs(x) - lambd, torch.zeros_like(x))\n",
    "\n",
    "def hard_threshold(x, lambd):\n",
    "    return x * (torch.abs(x) > lambd).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLCSC6 returns the reconstructed image from the latent space (n_batch, 1024, 2, 2)\n",
    "class MLCSC6(nn.Module):\n",
    "    def __init__(self, n_layers = 6):\n",
    "        super(MLCSC6, self).__init__()\n",
    "        \n",
    "        D1 = nn.ConvTranspose2d(128, 1, kernel_size=2, stride=2, padding=0)\n",
    "        D2 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=0)\n",
    "        D3 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=0)\n",
    "        D4 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=0)\n",
    "        D5 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=0)\n",
    "        D6 = nn.ConvTranspose2d(256, 256, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        #self.layers = [D6, D5, D4, D3, D2, D1]\n",
    "        self.layers = [D6, D5, D4, D3, D2, D1]\n",
    "        self.n_layers = n_layers\n",
    "        self.layers = nn.ModuleList(self.layers[6-n_layers:])\n",
    "        self.strides = [1, 1, 1, 1, 1, 2][6-n_layers:]\n",
    "        self.ksizes = [5, 3, 3, 3, 3, 6][6-n_layers:]\n",
    "    \n",
    "    def F(self, x, lambd = 0.1):\n",
    "        return torch.div(torch.square(x).flatten(1).sum(1), 2) + lambd * torch.abs(x).flatten(1).sum(1)\n",
    "    \n",
    "    def initialize(self, x):\n",
    "        \n",
    "        for i in range(self.n_layers-1, -1, -1):\n",
    "            x = F.conv2d(x, self.layers[i].weight, stride=self.strides[i], padding=0)\n",
    "            #x = self.layers[i](x)\n",
    "            #print(x.shape)\n",
    "        #pass\n",
    "        return x\n",
    "    \n",
    "    def FISTA(self, x, y, lambd = 0.03, n_iter = 20):\n",
    "        batch_size = x.size(0)\n",
    "        Dx = self.forward(x)\n",
    "        \n",
    "        gradient = Dx-y\n",
    "        for i in range(self.n_layers, 0, -1):\n",
    "            gradient = F.conv2d(gradient, self.layers[i-1].weight, stride=self.strides[i-1], padding=0)\n",
    "        tk, tk_next = torch.tensor(1., device = x.device), torch.tensor(1., device = x.device)\n",
    "        loss_list = []\n",
    "        for _ in range(n_iter):\n",
    "            z = x.clone()\n",
    "            const = self.F(z, lambd).reshape(-1, 1, 1, 1)\n",
    "            \n",
    "            L = torch.ones((batch_size, 1, 1, 1), device = x.device)\n",
    "            stop_line_search = torch.zeros((batch_size), device=x.device).bool()\n",
    "            while torch.sum(stop_line_search) < batch_size:\n",
    "                # line search\n",
    "                # print(z.shape, gradient.shape, L.shape)\n",
    "                prox_z = soft_threshold(z - torch.div(gradient, L), torch.div(lambd, L))\n",
    "                \n",
    "                # check descent condition\n",
    "                temp1 = self.F(prox_z, lambd).reshape(-1, 1, 1, 1)\n",
    "                temp2 = const + torch.mul(gradient, prox_z - z).flatten(1).sum(1).reshape(-1, 1, 1, 1) + \\\n",
    "                                torch.div(L, 2) * torch.square(prox_z - z).flatten(1).sum(1).reshape(-1, 1, 1, 1)\n",
    "                stop_line_search = temp1 <= temp2\n",
    "                L = torch.where(stop_line_search, L, 2 * L)\n",
    "            \n",
    "            tk_next = (1 + torch.sqrt(1 + 4 * tk**2)) / 2\n",
    "            x = prox_z + torch.div(tk - 1, tk_next) * (prox_z - z)\n",
    "            tk = tk_next\n",
    "            loss_list.append(torch.mean(self.F(x, lambd)).item())\n",
    "        \n",
    "        return x, loss_list\n",
    "    \n",
    "    def IHT(self, lambds):\n",
    "        # lambd2, lambd3 = 0.005, 0.01#0.01, 0.005, 0.01\n",
    "        # self.layer1.weight = hard_threshold(self.layer1.weight, lambd1)\n",
    "        # self.layer2.weight = nn.Parameter(hard_threshold(self.layer2.weight, lambd2))\n",
    "        # self.layer3.weight = nn.Parameter(hard_threshold(self.layer3.weight, lambd3))\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.layers[i].weight = nn.Parameter(hard_threshold(self.layers[i].weight, lambds[i-1]))\n",
    "            #print(torch.sum(self.layers[i].weight.flatten()==0) / torch.numel(self.layers[i].weight))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x)\n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.1596 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.2817 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.2769 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.2573 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.2320 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.2153 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.2146 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.2028 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1875 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1773 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1705 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1704 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1630 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1592 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1467 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1534 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1506 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1360 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1450 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1381 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1412 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1304 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1333 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1313 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1258 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1321 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1273 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1261 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1282 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1318 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1248 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1265 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1283 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1242 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1324 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1226 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1234 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1266 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1215 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1245 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1202 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1207 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1224 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1286 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1271 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1263 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1235 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1207 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1218 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1333 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1249 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1192 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1174 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1310 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1175 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1228 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1292 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1260 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1163 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1184 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1222 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1311 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1269 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1209 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1250 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1201 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1216 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1214 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1198 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1238 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1245 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1203 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1193 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1244 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1202 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1158 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1229 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1192 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1164 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1242 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1187 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1230 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1183 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1184 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1192 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1227 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1224 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1130 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1277 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1209 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1172 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1264 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1222 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1247 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1231 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1265 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1169 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1181 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1227 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1178 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1196 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1290 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1167 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1177 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1161 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1184 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1239 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1164 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1230 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1173 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1241 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1164 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1238 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1205 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1224 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1236 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1233 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1175 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1167 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n",
      "loss 0.1119 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.16']\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLCSC6().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            code = model.initialize(data)\n",
    "            #x = torch.randn_like(code, device=device)\n",
    "            x, _ = model.FISTA(code, data, 0.1, 20)\n",
    "        optimizer.zero_grad()\n",
    "        x = x.clone().requires_grad_(True)\n",
    "        output = model(x)\n",
    "        loss = F.mse_loss(output, data)# + 0.001 * (torch.norm(model.layer2.weight, 'fro') + torch.norm(model.layer3.weight, 'fro') + torch.norm(model.layer1.weight, 'fro'))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # iterative hard thresholding step\n",
    "        #model.IHT()\n",
    "        model.IHT([0.001, 0.005, 0.01, 0.02, 0.1])\n",
    "        #model.normalize_weights()\n",
    "        if batch_idx % 100 == 0:\n",
    "            l1_ratio = [(torch.sum(model.layers[i].weight.flatten()==0) / torch.numel(model.layers[i].weight)).item() for i in range(1, model.n_layers)]\n",
    "            print(\"loss\", \"{:.4f}\".format(loss.item()), \"sparsity:\", [\"{0:0.2f}\".format(i) for i in l1_ratio])\n",
    "    print(f'epoch {epoch}, loss {loss.item()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 256, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.5299 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.2269 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1896 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1816 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1956 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1882 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 1, loss 0.2050610444446405\n",
      "\n",
      "loss 0.1821 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1875 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1780 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1856 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1831 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1891 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 2, loss 0.18365054282049337\n",
      "\n",
      "loss 0.1852 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1951 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1903 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1878 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1789 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1858 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 3, loss 0.18558407805860042\n",
      "\n",
      "loss 0.1846 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1997 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1801 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1804 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1807 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1932 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 4, loss 0.18604873118301232\n",
      "\n",
      "loss 0.1854 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1886 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1787 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1894 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1773 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1811 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 5, loss 0.18582434316476187\n",
      "\n",
      "loss 0.1904 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1780 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1918 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1873 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1858 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1761 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 6, loss 0.186157258177797\n",
      "\n",
      "loss 0.1822 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1874 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1721 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.2056 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.2013 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1895 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 7, loss 0.18589496510724227\n",
      "\n",
      "loss 0.1838 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1899 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1880 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1687 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1777 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1842 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 8, loss 0.1861104613294204\n",
      "\n",
      "loss 0.1870 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1887 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1837 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1793 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1829 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1825 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 9, loss 0.18600385022660096\n",
      "\n",
      "loss 0.2018 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1897 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1915 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1736 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1875 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1940 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 10, loss 0.18598835232357183\n",
      "\n",
      "loss 0.1801 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1645 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.2024 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1896 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1947 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1764 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 11, loss 0.18627529489497344\n",
      "\n",
      "loss 0.1879 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1726 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1806 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1774 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1820 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1769 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "epoch 12, loss 0.1859792259335518\n",
      "\n",
      "loss 0.1873 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1937 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1843 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n",
      "loss 0.1760 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.40']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# code = model.initialize(data)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     code \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 22\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFISTA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m, in \u001b[0;36mMLCSC6.FISTA\u001b[0;34m(self, x, y, lambd, n_iter)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# check descent condition\u001b[39;00m\n\u001b[1;32m     53\u001b[0m temp1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mF(prox_z, lambd)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m temp2 \u001b[38;5;241m=\u001b[39m const \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(gradient, prox_z \u001b[38;5;241m-\u001b[39m z)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m---> 55\u001b[0m                 torch\u001b[38;5;241m.\u001b[39mdiv(L, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprox_z\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m stop_line_search \u001b[38;5;241m=\u001b[39m temp1 \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m temp2\n\u001b[1;32m     57\u001b[0m L \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(stop_line_search, L, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m L)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLCSC6().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # code = model.initialize(data)\n",
    "            code = torch.randn((100, 256, 2, 2), device=device)\n",
    "            x, _ = model.FISTA(code, data, 0.1, 20)\n",
    "        optimizer.zero_grad()\n",
    "        x = x.clone().requires_grad_(True)\n",
    "        output = model(x)\n",
    "        loss = F.mse_loss(output, data)# + 0.001 * (torch.norm(model.layer2.weight, 'fro') + torch.norm(model.layer3.weight, 'fro') + torch.norm(model.layer1.weight, 'fro'))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "        # iterative hard thresholding step\n",
    "        #model.IHT()\n",
    "        with torch.no_grad():\n",
    "            model.IHT([0.001, 0.005, 0.01, 0.02, 0.2])\n",
    "        #model.normalize_weights()\n",
    "        if batch_idx % 100 == 0:\n",
    "            l1_ratio = [(torch.sum(model.layers[i].weight.flatten()==0) / torch.numel(model.layers[i].weight)).item() for i in range(1, model.n_layers)]\n",
    "            print(\"loss\", \"{:.4f}\".format(loss.item()), \"sparsity:\", [\"{0:0.2f}\".format(i) for i in l1_ratio])\n",
    "    print(f'epoch {epoch}, loss {np.mean(loss_list)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'model_MNIST6.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020546875\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output.shape\n",
    "idx = 56\n",
    "#plt.imshow(output[idx].cpu().detach().numpy().reshape(28, 28), cmap='gray')\n",
    "#plt.show()\n",
    "#plt.imshow(data[idx].cpu().detach().numpy().reshape(28, 28), cmap='gray')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].imshow(output[idx].cpu().detach().numpy().reshape(28, 28), cmap='gray')\n",
    "ax[1].imshow(data[idx].cpu().detach().numpy().reshape(28, 28), cmap='gray')\n",
    "'''\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9, 3))\n",
    "ax[0].hist(model.layer2.weight.cpu().detach().numpy().flatten(), bins=200)\n",
    "ax[1].hist(model.layer3.weight.cpu().detach().numpy().flatten(), bins=200)\n",
    "ax[2].hist(x.cpu().detach().numpy().flatten(), bins=200)\n",
    "plt.show()\n",
    "'''\n",
    "print(np.sum(x.cpu().detach().numpy().flatten()==0) / x.cpu().detach().numpy().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "(60000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "output_list_train = []\n",
    "label_list_train = []\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=False)\n",
    "for batch_idx, (data, label) in enumerate(train_loader):\n",
    "    data = data.to(device)\n",
    "    print(batch_idx)\n",
    "    with torch.no_grad():\n",
    "        code = model.initialize(data)\n",
    "        x = torch.randn_like(code, device=device)\n",
    "        x, _ = model.FISTA(code, data, 0.1, 20)\n",
    "        # x = x.clone()\n",
    "        output = model(x)\n",
    "        output_list_train.append(output.cpu().detach().numpy())\n",
    "        label_list_train.append(label.cpu().detach().numpy())\n",
    "        \n",
    "output_list_train = np.concatenate(output_list_train, axis=0)\n",
    "print(output_list_train.shape)\n",
    "label_list_train = np.concatenate(label_list_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "#label_list_train = np.stack(label_list_train, axis=0)\n",
    "print(label_list_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "idx = 1234\n",
    "\n",
    "plt.imshow(output_list_train[idx].reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "print(label_list_train[idx])\n",
    "#torch.save(output_list_train, 'data/output_MNIST6_train.pt')\n",
    "#torch.save(label_list_train, 'data/label_MNIST6_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "(10000, 1, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "output_list_test = []\n",
    "label_list_test = []\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)\n",
    "for batch_idx, (data, label) in enumerate(test_loader):\n",
    "    data = data.to(device)\n",
    "    print(batch_idx)\n",
    "    with torch.no_grad():\n",
    "        code = model.initialize(data)\n",
    "        x = torch.randn_like(code, device=device)\n",
    "        x, _ = model.FISTA(code, data, 0.1, 20)\n",
    "        # x = x.clone()\n",
    "        output = model(x)\n",
    "        output_list_test.append(output.cpu().detach().numpy())\n",
    "        label_list_test.append(label.cpu().detach().numpy())\n",
    "        \n",
    "output_list_test = np.concatenate(output_list_test, axis=0)\n",
    "print(output_list_test.shape)\n",
    "label_list_test = np.concatenate(label_list_test, axis=0)\n",
    "print(label_list_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#label_list_test = np.concatenate(label_list_test, axis=0)\n",
    "print(label_list_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(output_list_test.shape)\n",
    "print(label_list_test.shape)\n",
    "torch.save(output_list_test, 'data/output_MNIST6_test.pt')\n",
    "torch.save(label_list_test, 'data/label_MNIST6_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list_train = torch.load('data/output_MNIST6_train.pt')\n",
    "label_list_train = torch.load('data/label_MNIST6_train.pt')\n",
    "output_list_test = torch.load('data/output_MNIST6_test.pt')\n",
    "label_list_test = torch.load('data/label_MNIST6_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n",
      "5\n",
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write a data loader for train_data and test_data\n",
    "stats = ((0.5), (0.5))\n",
    "class SparseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels, train=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        if not train:\n",
    "            self.transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(*stats,inplace=True),\n",
    "                                                ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.RandomCrop(28, padding=4, padding_mode='reflect'),\n",
    "                                                  transforms.RandomHorizontalFlip(),\n",
    "                                                  transforms.Normalize(*stats,inplace=True),\n",
    "                                                ])\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        img = (img - img.min())/(img.max() - img.min())\n",
    "        #else:\n",
    "        #    img = img\n",
    "        # img = (img - img.min())/(img.max() - img.min())\n",
    "        img = self.transform(img.transpose(1,2,0))\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train_sparse_dataset = SparseDataset(output_list_train, label_list_train, train=False)\n",
    "display, label = next(iter(train_sparse_dataset))\n",
    "plt.imshow(display.permute(1,2,0), cmap='gray')\n",
    "plt.axis('off')\n",
    "print(display.min(), display.max())\n",
    "print(label)\n",
    "print(display.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(2), \n",
    "                                        nn.Flatten(), \n",
    "                                        nn.Linear(512, num_classes))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.698\n",
      "[1,   400] loss: 0.338\n",
      "[2,   200] loss: 0.267\n",
      "[2,   400] loss: 0.244\n",
      "[3,   200] loss: 0.207\n",
      "[3,   400] loss: 0.197\n",
      "[4,   200] loss: 0.176\n",
      "[4,   400] loss: 0.187\n",
      "[5,   200] loss: 0.165\n",
      "[5,   400] loss: 0.167\n",
      "[6,   200] loss: 0.155\n",
      "[6,   400] loss: 0.145\n",
      "[7,   200] loss: 0.140\n",
      "[7,   400] loss: 0.139\n",
      "[8,   200] loss: 0.132\n",
      "[8,   400] loss: 0.136\n",
      "[9,   200] loss: 0.130\n",
      "[9,   400] loss: 0.125\n",
      "[10,   200] loss: 0.118\n",
      "[10,   400] loss: 0.121\n",
      "[11,   200] loss: 0.111\n",
      "[11,   400] loss: 0.118\n",
      "[12,   200] loss: 0.110\n",
      "[12,   400] loss: 0.107\n",
      "[13,   200] loss: 0.101\n",
      "[13,   400] loss: 0.102\n",
      "[14,   200] loss: 0.094\n",
      "[14,   400] loss: 0.103\n",
      "[15,   200] loss: 0.090\n",
      "[15,   400] loss: 0.098\n",
      "[16,   200] loss: 0.085\n",
      "[16,   400] loss: 0.089\n",
      "[17,   200] loss: 0.081\n",
      "[17,   400] loss: 0.087\n",
      "[18,   200] loss: 0.074\n",
      "[18,   400] loss: 0.083\n",
      "[19,   200] loss: 0.076\n",
      "[19,   400] loss: 0.080\n",
      "[20,   200] loss: 0.071\n",
      "[20,   400] loss: 0.082\n",
      "[21,   200] loss: 0.068\n",
      "[21,   400] loss: 0.074\n",
      "[22,   200] loss: 0.070\n",
      "[22,   400] loss: 0.066\n",
      "[23,   200] loss: 0.065\n",
      "[23,   400] loss: 0.067\n",
      "[24,   200] loss: 0.063\n",
      "[24,   400] loss: 0.065\n",
      "[25,   200] loss: 0.056\n",
      "[25,   400] loss: 0.069\n",
      "[26,   200] loss: 0.057\n",
      "[26,   400] loss: 0.061\n",
      "[27,   200] loss: 0.052\n",
      "[27,   400] loss: 0.061\n",
      "[28,   200] loss: 0.051\n",
      "[28,   400] loss: 0.055\n",
      "[29,   200] loss: 0.053\n",
      "[29,   400] loss: 0.051\n",
      "[30,   200] loss: 0.046\n",
      "[30,   400] loss: 0.058\n",
      "[31,   200] loss: 0.051\n",
      "[31,   400] loss: 0.052\n",
      "[32,   200] loss: 0.044\n",
      "[32,   400] loss: 0.050\n",
      "[33,   200] loss: 0.044\n",
      "[33,   400] loss: 0.047\n",
      "[34,   200] loss: 0.042\n",
      "[34,   400] loss: 0.045\n",
      "[35,   200] loss: 0.043\n",
      "[35,   400] loss: 0.044\n",
      "[36,   200] loss: 0.040\n",
      "[36,   400] loss: 0.040\n",
      "[37,   200] loss: 0.037\n",
      "[37,   400] loss: 0.039\n",
      "[38,   200] loss: 0.036\n",
      "[38,   400] loss: 0.042\n",
      "[39,   200] loss: 0.036\n",
      "[39,   400] loss: 0.041\n",
      "[40,   200] loss: 0.037\n",
      "[40,   400] loss: 0.039\n",
      "[41,   200] loss: 0.035\n",
      "[41,   400] loss: 0.039\n",
      "[42,   200] loss: 0.032\n",
      "[42,   400] loss: 0.036\n",
      "[43,   200] loss: 0.032\n",
      "[43,   400] loss: 0.037\n",
      "[44,   200] loss: 0.030\n",
      "[44,   400] loss: 0.034\n",
      "[45,   200] loss: 0.027\n",
      "[45,   400] loss: 0.031\n",
      "[46,   200] loss: 0.031\n",
      "[46,   400] loss: 0.031\n",
      "[47,   200] loss: 0.029\n",
      "[47,   400] loss: 0.031\n",
      "[48,   200] loss: 0.026\n",
      "[48,   400] loss: 0.030\n",
      "[49,   200] loss: 0.029\n",
      "[49,   400] loss: 0.031\n",
      "[50,   200] loss: 0.026\n",
      "[50,   400] loss: 0.028\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = ResNet9()\n",
    "net = net.to(device)\n",
    "# batch_size = 4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_sparse = SparseDataset(output_list_train, label_list_train, train=True)\n",
    "train_loader = DataLoader(train_sparse, batch_size=128, shuffle=True, num_workers=4)\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels.type(torch.LongTensor).cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "        \n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.14%\n"
     ]
    }
   ],
   "source": [
    "test_sparse = SparseDataset(output_list_test, label_list_test, train=False)\n",
    "test_loader = DataLoader(test_sparse, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.type(torch.LongTensor).cuda()).sum().item()\n",
    "        \n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
