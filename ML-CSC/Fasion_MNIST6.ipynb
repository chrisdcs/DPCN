{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import grad\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold(x, lambd):\n",
    "    return torch.sign(x) * torch.max(torch.abs(x) - lambd, torch.zeros_like(x))\n",
    "\n",
    "def hard_threshold(x, lambd):\n",
    "    return x * (torch.abs(x) > lambd).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLCSC6 returns the reconstructed image from the latent space (n_batch, 1024, 2, 2)\n",
    "class MLCSC6(nn.Module):\n",
    "    def __init__(self, n_layers = 6):\n",
    "        super(MLCSC6, self).__init__()\n",
    "        \n",
    "        D1 = nn.ConvTranspose2d(128, 1, kernel_size=2, stride=2, padding=0)\n",
    "        D2 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=0)\n",
    "        D3 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=0)\n",
    "        D4 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=0)\n",
    "        D5 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=0)\n",
    "        D6 = nn.ConvTranspose2d(256, 256, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        #self.layers = [D6, D5, D4, D3, D2, D1]\n",
    "        self.layers = [D6, D5, D4, D3, D2, D1]\n",
    "        self.n_layers = n_layers\n",
    "        self.layers = nn.ModuleList(self.layers[6-n_layers:])\n",
    "        self.strides = [1, 1, 1, 1, 1, 2][6-n_layers:]\n",
    "        self.ksizes = [5, 3, 3, 3, 3, 6][6-n_layers:]\n",
    "    \n",
    "    def F(self, x, lambd = 0.1):\n",
    "        return torch.div(torch.square(x).flatten(1).sum(1), 2) + lambd * torch.abs(x).flatten(1).sum(1)\n",
    "    \n",
    "    def initialize(self, x):\n",
    "        \n",
    "        for i in range(self.n_layers-1, -1, -1):\n",
    "            x = F.conv2d(x, self.layers[i].weight, stride=self.strides[i], padding=0)\n",
    "            #x = self.layers[i](x)\n",
    "            #print(x.shape)\n",
    "        #pass\n",
    "        return x\n",
    "    \n",
    "    def FISTA(self, x, y, lambd = 0.03, n_iter = 20):\n",
    "        batch_size = x.size(0)\n",
    "        Dx = self.forward(x)\n",
    "        \n",
    "        gradient = Dx-y\n",
    "        for i in range(self.n_layers, 0, -1):\n",
    "            gradient = F.conv2d(gradient, self.layers[i-1].weight, stride=self.strides[i-1], padding=0)\n",
    "        tk, tk_next = torch.tensor(1., device = x.device), torch.tensor(1., device = x.device)\n",
    "        loss_list = []\n",
    "        for _ in range(n_iter):\n",
    "            z = x.clone()\n",
    "            const = self.F(z, lambd).reshape(-1, 1, 1, 1)\n",
    "            \n",
    "            L = torch.ones((batch_size, 1, 1, 1), device = x.device)\n",
    "            stop_line_search = torch.zeros((batch_size), device=x.device).bool()\n",
    "            while torch.sum(stop_line_search) < batch_size:\n",
    "                # line search\n",
    "                # print(z.shape, gradient.shape, L.shape)\n",
    "                prox_z = soft_threshold(z - torch.div(gradient, L), torch.div(lambd, L))\n",
    "                \n",
    "                # check descent condition\n",
    "                temp1 = self.F(prox_z, lambd).reshape(-1, 1, 1, 1)\n",
    "                temp2 = const + torch.mul(gradient, prox_z - z).flatten(1).sum(1).reshape(-1, 1, 1, 1) + \\\n",
    "                                torch.div(L, 2) * torch.square(prox_z - z).flatten(1).sum(1).reshape(-1, 1, 1, 1)\n",
    "                stop_line_search = temp1 <= temp2\n",
    "                L = torch.where(stop_line_search, L, 2 * L)\n",
    "            \n",
    "            tk_next = (1 + torch.sqrt(1 + 4 * tk**2)) / 2\n",
    "            x = prox_z + torch.div(tk - 1, tk_next) * (prox_z - z)\n",
    "            tk = tk_next\n",
    "            loss_list.append(torch.mean(self.F(x, lambd)).item())\n",
    "        \n",
    "        return x, loss_list\n",
    "    \n",
    "    def IHT(self, lambds):\n",
    "        # lambd2, lambd3 = 0.005, 0.01#0.01, 0.005, 0.01\n",
    "        # self.layer1.weight = hard_threshold(self.layer1.weight, lambd1)\n",
    "        # self.layer2.weight = nn.Parameter(hard_threshold(self.layer2.weight, lambd2))\n",
    "        # self.layer3.weight = nn.Parameter(hard_threshold(self.layer3.weight, lambd3))\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.layers[i].weight = nn.Parameter(hard_threshold(self.layers[i].weight, lambds[i-1]))\n",
    "            #print(torch.sum(self.layers[i].weight.flatten()==0) / torch.numel(self.layers[i].weight))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x)\n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.8310 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.3268 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.3146 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.3099 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2711 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2724 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 1, loss 0.2563464641571045\n",
      "\n",
      "loss 0.2344 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2331 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2263 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2383 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2299 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2282 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 2, loss 0.2061709612607956\n",
      "\n",
      "loss 0.2417 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2281 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1973 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2189 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2128 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1994 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 3, loss 0.19206909835338593\n",
      "\n",
      "loss 0.1991 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1951 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2118 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2021 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1892 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2002 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 4, loss 0.1759072095155716\n",
      "\n",
      "loss 0.2000 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1908 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1860 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1779 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1876 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2016 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 5, loss 0.1882987767457962\n",
      "\n",
      "loss 0.1967 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1866 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1896 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1912 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1786 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1797 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 6, loss 0.17558331787586212\n",
      "\n",
      "loss 0.1803 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1870 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1872 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1810 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1850 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1908 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 7, loss 0.18180830776691437\n",
      "\n",
      "loss 0.1846 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1842 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1717 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1801 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1825 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1929 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 8, loss 0.18629173934459686\n",
      "\n",
      "loss 0.1779 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1843 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1798 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1732 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1948 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1797 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 9, loss 0.19790320098400116\n",
      "\n",
      "loss 0.1801 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1883 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1876 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1673 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1812 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1706 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 10, loss 0.17613162100315094\n",
      "\n",
      "loss 0.1688 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1734 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1864 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1697 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1918 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1803 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 11, loss 0.18659526109695435\n",
      "\n",
      "loss 0.1910 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1863 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1927 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1777 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1778 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1876 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 12, loss 0.17795594036579132\n",
      "\n",
      "loss 0.1775 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1912 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1851 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1834 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1788 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1838 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 13, loss 0.17581020295619965\n",
      "\n",
      "loss 0.1824 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2036 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1864 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1837 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1969 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.2022 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 14, loss 0.17935609817504883\n",
      "\n",
      "loss 0.1898 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1768 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1779 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1822 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1893 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1847 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 15, loss 0.1679553985595703\n",
      "\n",
      "loss 0.1881 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1918 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1940 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1882 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1863 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1799 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 16, loss 0.1876915693283081\n",
      "\n",
      "loss 0.1921 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1818 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1841 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1693 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1897 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1826 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 17, loss 0.1660783290863037\n",
      "\n",
      "loss 0.1945 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1868 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1845 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1824 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1808 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1937 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 18, loss 0.1892799288034439\n",
      "\n",
      "loss 0.1861 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1871 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1872 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1856 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1797 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1769 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 19, loss 0.19152012467384338\n",
      "\n",
      "loss 0.1670 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1757 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1776 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1849 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1799 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "loss 0.1816 sparsity: ['0.03', '0.17', '0.34', '0.68', '0.21']\n",
      "epoch 20, loss 0.189566507935524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_set = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLCSC6().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            code = model.initialize(data)\n",
    "            #x = torch.randn_like(code, device=device)\n",
    "            x, _ = model.FISTA(code, data, 0.03, 10)\n",
    "        optimizer.zero_grad()\n",
    "        x = x.clone().requires_grad_(True)\n",
    "        output = model(x)\n",
    "        loss = F.mse_loss(output, data)# + 0.001 * (torch.norm(model.layer2.weight, 'fro') + torch.norm(model.layer3.weight, 'fro') + torch.norm(model.layer1.weight, 'fro'))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # iterative hard thresholding step\n",
    "        #model.IHT()\n",
    "        model.IHT([0.001, 0.005, 0.01, 0.02, 0.1])\n",
    "        #model.normalize_weights()\n",
    "        if batch_idx % 100 == 0:\n",
    "            l1_ratio = [(torch.sum(model.layers[i].weight.flatten()==0) / torch.numel(model.layers[i].weight)).item() for i in range(1, model.n_layers)]\n",
    "            print(\"loss\", \"{:.4f}\".format(loss.item()), \"sparsity:\", [\"{0:0.2f}\".format(i) for i in l1_ratio])\n",
    "    print(f'epoch {epoch}, loss {loss.item()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004794921875\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output.shape\n",
    "idx = 20\n",
    "#plt.imshow(output[idx].cpu().detach().numpy().reshape(28, 28), cmap='gray')\n",
    "#plt.show()\n",
    "#plt.imshow(data[idx].cpu().detach().numpy().reshape(28, 28), cmap='gray')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].imshow(output[idx].cpu().detach().numpy().reshape(28, 28), cmap='gray')\n",
    "ax[1].imshow(data[idx].cpu().detach().numpy().reshape(28, 28), cmap='gray')\n",
    "'''\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9, 3))\n",
    "ax[0].hist(model.layer2.weight.cpu().detach().numpy().flatten(), bins=200)\n",
    "ax[1].hist(model.layer3.weight.cpu().detach().numpy().flatten(), bins=200)\n",
    "ax[2].hist(x.cpu().detach().numpy().flatten(), bins=200)\n",
    "plt.show()\n",
    "'''\n",
    "print(np.sum(x.cpu().detach().numpy().flatten()==0) / x.cpu().detach().numpy().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "(60000, 1, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "output_list_train = []\n",
    "label_list_train = []\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=False)\n",
    "for batch_idx, (data, label) in enumerate(train_loader):\n",
    "    data = data.to(device)\n",
    "    print(batch_idx)\n",
    "    with torch.no_grad():\n",
    "        code = model.initialize(data)\n",
    "        #x = torch.randn_like(code, device=device)\n",
    "        x, _ = model.FISTA(code, data, 0.03, 10)\n",
    "        # x = x.clone()\n",
    "        output = model(x)\n",
    "        output_list_train.append(output.cpu().detach().numpy())\n",
    "        label_list_train.append(label.cpu().detach().numpy())\n",
    "        \n",
    "output_list_train = np.concatenate(output_list_train, axis=0)\n",
    "print(output_list_train.shape)\n",
    "label_list_train = np.concatenate(label_list_train, axis=0)\n",
    "print(label_list_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "idx = 1234\n",
    "\n",
    "plt.imshow(output_list_train[idx].reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "print(label_list_train[idx])\n",
    "#torch.save(output_list_train, 'data/output_MNIST6_train.pt')\n",
    "#torch.save(label_list_train, 'data/label_MNIST6_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "(10000, 1, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "output_list_test = []\n",
    "label_list_test = []\n",
    "test_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)\n",
    "for batch_idx, (data, label) in enumerate(test_loader):\n",
    "    data = data.to(device)\n",
    "    print(batch_idx)\n",
    "    with torch.no_grad():\n",
    "        code = model.initialize(data)\n",
    "        x = torch.randn_like(code, device=device)\n",
    "        x, _ = model.FISTA(code, data, 0.03, 10)\n",
    "        # x = x.clone()\n",
    "        output = model(x)\n",
    "        output_list_test.append(output.cpu().detach().numpy())\n",
    "        label_list_test.append(label.cpu().detach().numpy())\n",
    "        \n",
    "output_list_test = np.concatenate(output_list_test, axis=0)\n",
    "print(output_list_test.shape)\n",
    "label_list_test = np.concatenate(label_list_test, axis=0)\n",
    "print(label_list_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n",
      "9\n",
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write a data loader for train_data and test_data\n",
    "stats = ((0.5), (0.5))\n",
    "class SparseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels, train=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        if not train:\n",
    "            self.transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(*stats,inplace=True),\n",
    "                                                ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.RandomCrop(28, padding=4, padding_mode='reflect'),\n",
    "                                                  transforms.RandomHorizontalFlip(),\n",
    "                                                  transforms.Normalize(*stats,inplace=True),\n",
    "                                                ])\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        img = (img - img.min())/(img.max() - img.min())\n",
    "        #else:\n",
    "        #    img = img\n",
    "        # img = (img - img.min())/(img.max() - img.min())\n",
    "        img = self.transform(img.transpose(1,2,0))\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train_sparse_dataset = SparseDataset(output_list_train, label_list_train, train=False)\n",
    "display, label = next(iter(train_sparse_dataset))\n",
    "plt.imshow(display.permute(1,2,0), cmap='gray')\n",
    "plt.axis('off')\n",
    "print(display.min(), display.max())\n",
    "print(label)\n",
    "print(display.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(2), \n",
    "                                        nn.Flatten(), \n",
    "                                        nn.Linear(512, num_classes))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.949\n",
      "[1,   400] loss: 0.789\n",
      "[2,   200] loss: 0.728\n",
      "[2,   400] loss: 0.693\n",
      "[3,   200] loss: 0.678\n",
      "[3,   400] loss: 0.659\n",
      "[4,   200] loss: 0.629\n",
      "[4,   400] loss: 0.627\n",
      "[5,   200] loss: 0.617\n",
      "[5,   400] loss: 0.602\n",
      "[6,   200] loss: 0.587\n",
      "[6,   400] loss: 0.588\n",
      "[7,   200] loss: 0.573\n",
      "[7,   400] loss: 0.575\n",
      "[8,   200] loss: 0.563\n",
      "[8,   400] loss: 0.553\n",
      "[9,   200] loss: 0.550\n",
      "[9,   400] loss: 0.545\n",
      "[10,   200] loss: 0.542\n",
      "[10,   400] loss: 0.546\n",
      "[11,   200] loss: 0.531\n",
      "[11,   400] loss: 0.527\n",
      "[12,   200] loss: 0.519\n",
      "[12,   400] loss: 0.513\n",
      "[13,   200] loss: 0.513\n",
      "[13,   400] loss: 0.510\n",
      "[14,   200] loss: 0.510\n",
      "[14,   400] loss: 0.507\n",
      "[15,   200] loss: 0.499\n",
      "[15,   400] loss: 0.501\n",
      "[16,   200] loss: 0.498\n",
      "[16,   400] loss: 0.490\n",
      "[17,   200] loss: 0.484\n",
      "[17,   400] loss: 0.479\n",
      "[18,   200] loss: 0.480\n",
      "[18,   400] loss: 0.483\n",
      "[19,   200] loss: 0.471\n",
      "[19,   400] loss: 0.479\n",
      "[20,   200] loss: 0.472\n",
      "[20,   400] loss: 0.474\n",
      "[21,   200] loss: 0.463\n",
      "[21,   400] loss: 0.467\n",
      "[22,   200] loss: 0.459\n",
      "[22,   400] loss: 0.462\n",
      "[23,   200] loss: 0.457\n",
      "[23,   400] loss: 0.453\n",
      "[24,   200] loss: 0.448\n",
      "[24,   400] loss: 0.451\n",
      "[25,   200] loss: 0.448\n",
      "[25,   400] loss: 0.454\n",
      "[26,   200] loss: 0.448\n",
      "[26,   400] loss: 0.441\n",
      "[27,   200] loss: 0.431\n",
      "[27,   400] loss: 0.449\n",
      "[28,   200] loss: 0.431\n",
      "[28,   400] loss: 0.441\n",
      "[29,   200] loss: 0.432\n",
      "[29,   400] loss: 0.429\n",
      "[30,   200] loss: 0.427\n",
      "[30,   400] loss: 0.431\n",
      "[31,   200] loss: 0.419\n",
      "[31,   400] loss: 0.423\n",
      "[32,   200] loss: 0.417\n",
      "[32,   400] loss: 0.424\n",
      "[33,   200] loss: 0.415\n",
      "[33,   400] loss: 0.416\n",
      "[34,   200] loss: 0.414\n",
      "[34,   400] loss: 0.416\n",
      "[35,   200] loss: 0.411\n",
      "[35,   400] loss: 0.413\n",
      "[36,   200] loss: 0.407\n",
      "[36,   400] loss: 0.405\n",
      "[37,   200] loss: 0.399\n",
      "[37,   400] loss: 0.408\n",
      "[38,   200] loss: 0.396\n",
      "[38,   400] loss: 0.400\n",
      "[39,   200] loss: 0.398\n",
      "[39,   400] loss: 0.389\n",
      "[40,   200] loss: 0.390\n",
      "[40,   400] loss: 0.397\n",
      "[41,   200] loss: 0.388\n",
      "[41,   400] loss: 0.389\n",
      "[42,   200] loss: 0.377\n",
      "[42,   400] loss: 0.392\n",
      "[43,   200] loss: 0.374\n",
      "[43,   400] loss: 0.384\n",
      "[44,   200] loss: 0.375\n",
      "[44,   400] loss: 0.379\n",
      "[45,   200] loss: 0.362\n",
      "[45,   400] loss: 0.385\n",
      "[46,   200] loss: 0.367\n",
      "[46,   400] loss: 0.374\n",
      "[47,   200] loss: 0.366\n",
      "[47,   400] loss: 0.370\n",
      "[48,   200] loss: 0.359\n",
      "[48,   400] loss: 0.367\n",
      "[49,   200] loss: 0.348\n",
      "[49,   400] loss: 0.368\n",
      "[50,   200] loss: 0.347\n",
      "[50,   400] loss: 0.366\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = ResNet9()\n",
    "net = net.to(device)\n",
    "# batch_size = 4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_sparse = SparseDataset(output_list_train, label_list_train, train=True)\n",
    "train_loader = DataLoader(train_sparse, batch_size=128, shuffle=True, num_workers=4)\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels.type(torch.LongTensor).cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "        \n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 82.33%\n"
     ]
    }
   ],
   "source": [
    "test_sparse = SparseDataset(output_list_test, label_list_test, train=False)\n",
    "test_loader = DataLoader(test_sparse, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.type(torch.LongTensor).cuda()).sum().item()\n",
    "        \n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
